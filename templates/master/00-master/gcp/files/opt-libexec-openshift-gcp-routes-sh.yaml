mode: 0755
path: "/opt/libexec/openshift-gcp-routes.sh"
contents:
  inline: |
    #!/bin/bash

    # Update iptables rules based on google cloud load balancer VIPS
    #
    # This is needed because the GCP L3 load balancer doesn't actually do DNAT;
    # the destination IP address is still the VIP. Normally, there is an agent that
    # adds the vip to the local routing table, tricking the kernel in to thinking
    # it's a local IP and allowing processes doing an accept(0.0.0.0) to receive
    # the packets. Clever.
    #
    # We don't do that. Instead, we DNAT with conntrack. This is so we don't break
    # existing connections when the vip is removed. This is useful for draining
    # connections - take ourselves out of the vip, but service existing conns.
    #
    # Additionally, clients can write a file to /run/cloud-routes/$IP.down to force
    # a VIP as down. This is useful for graceful shutdown / upgrade.
    #
    # ~cdc~

    set -e

    # the list of load balancer IPs that are assigned to this node
    declare -A vips

    curler() {
      # if the curl succeeds, write the body to stdout for the caller
      # to consume.
      #
      # if the curl fails, write the body to stderr so its in the
      # logs, but do not write anything to stdout.  The loop that the
      # curler call was feeding will bail with no results, and we'll
      # come back in on the next loop and try again.
      RESPONSE="$(curl --silent --show-error -L -H "Metadata-Flavor: Google" -w '\n%{response_code}' "http://metadata.google.internal/computeMetadata/v1/instance/${1}")" &&
        RESPONSE_CODE="$(echo "${RESPONSE}" | tail -n 1)" &&
        BODY="$(echo "${RESPONSE}" | head -n -1)" &&
        if test 0 -eq "${RESPONSE_CODE}" -o 400 -le "${RESPONSE_CODE}"; then
          printf "%s" "${BODY}" >&2
        else
          printf "%s" "${BODY}"
        fi
    }

    CHAIN_NAME="gcp-vips"
    RUN_DIR="/run/cloud-routes"

    # Create a chan if it doesn't exist
    ensure_chain() {
        local table="${1}"
        local chain="${2}"

        if ! iptables -w -t "${table}" -S "${chain}" &> /dev/null ; then
            iptables -w -t "${table}" -N "${chain}";
        fi;
    }

    ensure_rule() {
        local table="${1}"
        local chain="${2}"
        shift 2

        if ! iptables -w -t "${table}" -C "${chain}" "$@" &> /dev/null; then
            iptables -w -t "${table}" -A "${chain}" "$@"
        fi
    }

    # set the chain, ensure entry rules, ensure ESTABLISHED rule
    initialize() {
        ensure_chain nat "${CHAIN_NAME}"
        ensure_chain nat "${CHAIN_NAME}-local"
        ensure_rule nat PREROUTING -m comment --comment 'gcp LB vip DNAT' -j ${CHAIN_NAME}
        ensure_rule nat OUTPUT -m comment --comment 'gcp LB vip DNAT for local clients' -j ${CHAIN_NAME}-local

        # Need this so that existing flows (with an entry in conntrack) continue to be
        # balanced, even if the DNAT entry is removed
        ensure_rule filter INPUT -m comment --comment 'gcp LB vip existing' -m addrtype ! --dst-type LOCAL -m state --state ESTABLISHED,RELATED -j ACCEPT

        # bz1925698: GCP LBs can create stale entries causing apiservers disruption
        # The GCP LB Health Check polls continuously the LB VIP assigned to the VM
        # but, if the LB VIP is down, we are not handling the VIP traffic and
        # the traffic can hairpin, leaving stale conntrack entries on the host.
        # xref: https://bugzilla.redhat.com/show_bug.cgi?id=1925698#c29
        # Deleting conntrack entries solves the problem, but it also affects other connections
        # directed to the LB vips, causing unexpected networks disruptions.
        # The solution is to not FORWARD GCP HealthCheckers traffic, because that traffic 
        # is only directed to the host, and it must go to the INPUT chain.
        # xref: https://bugzilla.redhat.com/show_bug.cgi?id=1930457#c8
        # The HealthCheck origin ip-ranges are documented: 130.211.0.0/22 and 35.191.0.0/16
        # xref: https://cloud.google.com/load-balancing/docs/health-check-concepts#ip-ranges
        ensure_rule filter FORWARD -m comment --comment 'gcp HealthCheck traffic' -s 35.191.0.0/16 -j DROP
        ensure_rule filter FORWARD -m comment --comment 'gcp HealthCheck traffic' -s 130.211.0.0/22 -j DROP

        mkdir -p "${RUN_DIR}"
    }

    remove_stale() {
        ## find extra iptables rules
        for ipt_vip in $(iptables -w -t nat -S "${CHAIN_NAME}" | awk '$4{print $4}' | awk -F/ '{print $1}'); do
            if [[ -z "${vips[${ipt_vip}]}" ]]; then
                echo removing stale vip "${ipt_vip}" for external clients
                iptables -w -t nat -D "${CHAIN_NAME}" --dst "${ipt_vip}" -j REDIRECT
            fi
        done
        for ipt_vip in $(iptables -w -t nat -S "${CHAIN_NAME}-local" | awk '$4{print $4}' | awk -F/ '{print $1}'); do
            if [[ -z "${vips[${ipt_vip}]}" ]] || [[ "${vips[${ipt_vip}]}" = down ]]; then
                echo removing stale vip "${ipt_vip}" for local clients
                iptables -w -t nat -D "${CHAIN_NAME}-local" --dst "${ipt_vip}" -j REDIRECT
            fi
        done
    }

    remove_stale_routes() {
        ## find extra ovn routes
        local ovnkContainerID=$(crictl ps --name ovnkube-controller | awk '{ print $1 }' | tail -n+2)
        if [ -z "${ovnkContainerID}" ]; then
            echo "Plugin is SDN, nothing to do.. exiting"
            return
        fi
        echo "Found ovnkube-controller pod... ${ovnkContainerID}"
        local routeVIPsV4=$(crictl exec -i ${ovnkContainerID} ovn-nbctl lr-policy-list ovn_cluster_router | grep "1010" | grep "ip4" | awk '$8{print $8}')
        echo "Found v4route vips: ${routeVIPsV4}"
        local host=$(hostname)
        echo ${host}
        for route_vip in ${routeVIPsV4}; do
            if [[ ! -v vips[${route_vip}] ]] || [[ "${vips[${route_vip}]}" = down ]]; then
                echo removing stale vip "${route_vip}" for local clients
                echo "ovn-nbctl lr-policy-del ovn_cluster_router 1010 inport == rtos-${host} && ip4.dst == ${route_vip}"
                crictl exec -i ${ovnkContainerID} ovn-nbctl lr-policy-del ovn_cluster_router 1010 "inport == \"rtos-${host}\" && ip4.dst == ${route_vip}"
            fi
        done
    }

    add_rules() {
        for vip in "${!vips[@]}"; do
            echo "ensuring rule for ${vip} for external clients"
            ensure_rule nat "${CHAIN_NAME}" --dst "${vip}" -j REDIRECT

            if [[ "${vips[${vip}]}" != down ]]; then
                echo "ensuring rule for ${vip} for internal clients"
                ensure_rule nat "${CHAIN_NAME}-local" --dst "${vip}" -j REDIRECT
            fi
        done
    }

    add_routes() {
        local ovnkContainerID=$(crictl ps --name ovnkube-controller | awk '{ print $1 }' | tail -n+2)
        if [ -z "${ovnkContainerID}" ]; then
            echo "Plugin is SDN, nothing to do.. exiting"
            return
        fi
        echo "Found ovnkube-controller pod... ${ovnkContainerID}"
        local ovnK8sMp0v4=$(ip -brief address show ovn-k8s-mp0 | awk '{print $3}' | awk -F/ '{print $1}')
        echo "Found ovn-k8s-mp0 interface IP ${ovnK8sMp0v4}"
        local host=$(hostname)
        echo ${host}
        for vip in "${!vips[@]}"; do
            if [[ "${vips[${vip}]}" != down ]]; then
                echo "ensuring route for ${vip} for internal clients"
                local routes=$(crictl exec -i ${ovnkContainerID} ovn-nbctl lr-policy-list ovn_cluster_router | grep "1010" | grep "${vip}" | grep "${ovnK8sMp0v4}")
                echo "OVNK Routes on ovn-cluster-router at 1010 priority: $routes"
                if [[ "${routes}" == *"${vip}"* ]]; then
                    echo "Route exists"
                else
                    echo "Route does not exist; creating it..."
                    echo "ovn-nbctl lr-policy-add ovn_cluster_router 1010 inport == rtos-${host} && ip4.dst == ${vip} reroute ${ovnK8sMp0v4}"
                    crictl exec -i ${ovnkContainerID} ovn-nbctl lr-policy-add ovn_cluster_router 1010 "inport == \"rtos-${host}\" && ip4.dst == ${vip}" reroute "${ovnK8sMp0v4}"
                fi
            fi
        done
    }

    clear_rules() {
        iptables -t nat -F "${CHAIN_NAME}" || true
        iptables -t nat -F "${CHAIN_NAME}-local" || true
    }

    clear_routes() {
        local ovnkContainerID=$(crictl ps --name ovnkube-controller | awk '{ print $1 }' | tail -n+2)
        if [ -z "${ovnkContainerID}" ]; then
            echo "Plugin is SDN, nothing to do.. exiting"
            return
        fi
        echo "Found ovnkube-controller pod... ${ovnkContainerID}"
        echo "clearing all routes from ovn-cluster-router"
        crictl exec -i ${ovnkContainerID} ovn-nbctl lr-policy-del ovn_cluster_router 1010
    }

    # out paramater: vips
    list_lb_ips() {
        for k in "${!vips[@]}"; do
            unset vips["${k}"]
        done

        local net_path="network-interfaces/"
        for vif in $(curler ${net_path}); do
            local hw_addr; hw_addr=$(curler "${net_path}${vif}mac")
            local fwip_path; fwip_path="${net_path}${vif}forwarded-ips/"
            for level in $(curler "${fwip_path}"); do
                for fwip in $(curler "${fwip_path}${level}"); do
                    if [[ -e "${RUN_DIR}/${fwip}.down" ]]; then
                        echo "${fwip} is manually marked as down, skipping for internal clients..."
                        vips[${fwip}]="down"
                    else
                        echo "Processing route for NIC ${vif}${hw_addr} for ${fwip}"
                        vips[${fwip}]="${fwip}"
                    fi
                done
            done
        done
    }

    sleep_or_watch() {
        if hash inotifywait &> /dev/null; then
            inotifywait -t 30 -r "${RUN_DIR}" &> /dev/null || true
        else
            # no inotify, need to manually poll
            for i in {0..5}; do
                for vip in "${!vips[@]}"; do
                    if [[ "${vips[${vip}]}" != down ]] && [[ -e "${RUN_DIR}/${vip}.down" ]]; then
                        echo "new downfile detected"
                        break 2
                    elif [[ "${vips[${vip}]}" = down ]] && ! [[ -e "${RUN_DIR}/${vip}.down" ]]; then
                        echo "downfile disappeared"
                        break 2
                    fi
                done
                sleep 1 # keep this small enough to not make gcp-routes slower than LBs on recovery
            done
        fi
    }

    case "$1" in
      start)
        initialize
        while :; do
          list_lb_ips
          remove_stale
          remove_stale_routes # needed for OVN-Kubernetes plugin's routingViaHost=false mode
          add_rules
          add_routes # needed for OVN-Kubernetes plugin's routingViaHost=false mode
          echo "done applying vip rules"
          sleep_or_watch
        done
        ;;
      cleanup)
        clear_rules
        clear_routes # needed for OVN-Kubernetes plugin's routingViaHost=false mode
        ;;
      *)
        echo $"Usage: $0 {start|cleanup}"
        exit 1
    esac
